import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from data_adapter import DataAdapter
from feature_utils import prepare_features, get_feature_columns, generate_labels, generate_labels_from_csv
import numpy as np
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm
import datetime
import pandas as pd
import json
import os
import matplotlib.pyplot as plt
import copy
from torch.serialization import default_restore_location
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False
from pandas.api.types import is_numeric_dtype

class SOLDataset(Dataset):
    def __init__(self, features, labels):
        self.features = torch.FloatTensor(features)
        self.labels = torch.FloatTensor(labels)
        
    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

class EnhancedSOLModel(nn.Module):
    def __init__(self, input_size):
        super(EnhancedSOLModel, self).__init__()
        # ä¿®æ”¹LSTMè¾“å‡ºç»´åº¦
        self.lstm = nn.LSTM(input_size, 64,  # è¿›ä¸€æ­¥å‡å°‘éšè—å±‚ç»´åº¦
                          num_layers=2,  # å‡å°‘å±‚æ•°
                          batch_first=True,
                          bidirectional=True,
                          dropout=0.3)
        
        # è°ƒæ•´æ³¨æ„åŠ›ç»´åº¦
        self.seq_dropout = nn.Dropout2d(0.2)
        self.attention = nn.MultiheadAttention(embed_dim=128,  # åŒ¹é…LSTMè¾“å‡º
                                             num_heads=4,
                                             dropout=0.2)
        
       # åœ¨ä¿¡å·å¤´å¢åŠ æ›´å¤šDropout
        self.signal_head = nn.Sequential(
            nn.Linear(128, 64),
            nn.LeakyReLU(0.1),
            nn.Dropout(0.5),  # ä»0.4å¢åŠ åˆ°0.5
            nn.BatchNorm1d(64),
            nn.Dropout(0.4),  # æ–°å¢ç¬¬äºŒå±‚Dropout
            nn.Linear(64, 1)
        )

    def _init_weights(self):
        for name, param in self.named_parameters():
            if 'weight' in name and param.dim() > 1:
                nn.init.xavier_uniform_(param)  # æ”¹ç”¨Xavieråˆå§‹åŒ–
            elif 'bias' in name:
                nn.init.constant_(param, 0.0)

    def forward(self, x):
        # è°ƒæ•´åˆå§‹åŒ–ç»´åº¦
        num_directions = 2 if self.lstm.bidirectional else 1
        h0 = torch.zeros(self.lstm.num_layers * num_directions, x.size(0), 64).to(x.device)
        c0 = torch.zeros(self.lstm.num_layers * num_directions, x.size(0), 64).to(x.device)
        
        lstm_out, _ = self.lstm(x, (h0, c0))  # ä¿æŒåŸæœ‰ä»£ç ä¸å˜
        
        # ä¿®æ­£æ³¨æ„åŠ›è®¡ç®—
        attn_out, _ = self.attention(
            lstm_out.permute(1,0,2),
            lstm_out.permute(1,0,2),
            lstm_out.permute(1,0,2)
        )
        context = attn_out.mean(dim=0)
        signal = self.signal_head(context)
        return signal

class SOLTrainer:
    def __init__(self, config):
        # æ–°å¢éšæœºç§å­è®¾ç½®
        seed = 42
        torch.manual_seed(seed)
        np.random.seed(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        self.data_adapter = DataAdapter(
            source=config['source'],
            path=config['data_path'],
            mode='backtest'
        )
        torch.serialization.add_safe_globals([np.core.multiarray._reconstruct])
        self.timeframe = config['timeframe']
        self.batch_size = config.get('batch_size', 256)
        self.seq_length = config.get('seq_length', 24)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.best_return = -np.inf
        self.best_loss = np.inf
        self.model_version = 1
        self.existing_model = config.get('existing_model')
        # æ›¿æ¢åŸæœ‰æ—¶é—´è®¾ç½®
        self.cv_config = config.get('cv_config', {})
        self.current_fold = 0
        self.total_folds = self.cv_config.get('num_folds', 5)
        self._init_time_windows(config)
        self.future_window = config.get('future_window', 1)  # ä»é…ç½®è¯»å–
        self.scaler = StandardScaler()
        # æ–°å¢éªŒè¯ç»“æœè·Ÿè¸ª
        self.best_val_returns = []
        self.best_epochs = []
        self.loss_weights = {
            'signal': config.get('signal_weight', 1), #1.0
            'return': config.get('return_weight', 25),  # é»˜è®¤æé«˜æ”¶ç›Šé¡¹æƒé‡ï¼Œ #50
            'smooth': config.get('smooth_weight', 0),    # æé«˜å¹³æ»‘é¡¹æƒé‡ #2
            'drawdown': config.get('drawdown_weight', 0)  # æ–°å¢å›æ’¤æƒ©ç½šé¡¹
        }

         # æ–°å¢æ—©åœé…ç½®
        self.early_stop_patience = config.get('early_stop_patience', 300)  # é»˜è®¤7ä¸ªepochæ— æ”¹è¿›åœæ­¢
        self.no_improve_epochs = 0
        self.best_val_acc = 0
        self.best_val_return = -np.inf
        self.feature_importance_history = []  # æ–°å¢ç‰¹å¾é‡è¦æ€§è®°å½•
        self.feature_blacklist = set()  # æ–°å¢æ— æ•ˆç‰¹å¾é»‘åå•
        self.conservative_rate = config.get('conservative_rate', 1)
        # æ–°å¢ç»˜å›¾é…ç½®
        self.plot_validation = config.get('plot_validation', False)  # é»˜è®¤å…³é—­ç»˜å›¾
        self.initial_balance = 10000  # åˆå§‹æœ¬é‡‘
        self.save_best_checkpoint = config.get('save_best_checkpoint', True)  # æ–°å¢é…ç½®é¡¹
        self.best_checkpoint_path = ""  # æ–°å¢æœ€ä½³æ¨¡å‹è·¯å¾„è·Ÿè¸ª
        self.train_loader = None
        self.val_loader = None
        # æ–°å¢è‡ªåŠ¨è®­ç»ƒå‚æ•°
        self.auto_train_config = {
            'target_acc': 0.55,        # ç›®æ ‡éªŒè¯é›†å‡†ç¡®ç‡
            'max_drawdown': 0.1,       # æœ€å¤§å…è®¸å›æ’¤
            'lr_search_space': [0.001, 0.0005, 0.0001],  # å­¦ä¹ ç‡æœç´¢ç©ºé—´
            'weight_search_space': {    # æƒé‡æœç´¢ç©ºé—´
                'signal': [1, 2, 3],
                'return': [50, 100, 200],
                'smooth': [10, 30, 50]
            },
            'max_retries': 10           # æœ€å¤§å°è¯•æ¬¡æ•°
        }
        self.adaptive_config = {
            'max_dropout': 0.7,  # æœ€å¤§dropoutæ¦‚ç‡
            'weight_decay_range': [0.01, 0.1],  # æƒé‡è¡°å‡èŒƒå›´
            'noise_scale': 0.01  # è¾“å…¥å™ªå£°å¼ºåº¦
        }


    # æ–°å¢è‡ªåŠ¨è®­ç»ƒæ–¹æ³•
    def auto_train(self, symbol, initial_epochs=1):

        best_metrics = {
            'acc': 0, 
            'max_drawdown': 1.0,
            'weights': self.loss_weights.copy(),
            'model': None,
            'attempt': -1
        }
        attempt = 0
        saved_models = []
        model_dir = os.path.join("model", "auto_models", symbol)
        os.makedirs(model_dir, exist_ok=True)

        while attempt < self.auto_train_config['max_retries']:
            print(f"\n=== è‡ªåŠ¨è®­ç»ƒå°è¯• #{attempt+1} ===")
            
            # åŠ¨æ€è°ƒæ•´å‚æ•°
            current_lr = self.auto_train_config['lr_search_space'][attempt % len(self.auto_train_config['lr_search_space'])]
            self.loss_weights = {
                'signal': self.auto_train_config['weight_search_space']['signal'][attempt % 3],
                'return': self.auto_train_config['weight_search_space']['return'][attempt % 3],
                'smooth': self.auto_train_config['weight_search_space']['smooth'][attempt % 3],
                'drawdown': 0.5 if best_metrics['max_drawdown'] > 0.1 else 0
            }
            
            # æ‰§è¡Œè®­ç»ƒ
            model = self.train_model(symbol, epochs=initial_epochs, conservative_rate=0.8)
            val_result = self._evaluate_model(model, self.val_loader, is_val=True)
            
            # æ›´æ–°å…¨å±€æœ€ä½³æ¨¡å‹
            if (val_result['acc'] > best_metrics['acc'] or 
               (val_result['acc'] == best_metrics['acc'] and 
                val_result['max_drawdown'] < best_metrics['max_drawdown'])):
                
                best_metrics.update({
                    'acc': val_result['acc'],
                    'max_drawdown': val_result['max_drawdown'],
                    'weights': self.loss_weights.copy(),
                    'model': copy.deepcopy(model.state_dict()),  # ä¿å­˜æ¨¡å‹å‚æ•°
                    'attempt': attempt + 1
                })
                
                # ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆæ–°å¢æ—¶é—´æˆ³ï¼‰
                timestamp = datetime.datetime.now().strftime("%m%d%H%M")
                model_name = (f"{symbol}_best_acc{val_result['acc']:.2f}_"
                             f"dd{val_result['max_drawdown']:.2f}_{timestamp}.pth")
                checkpoint_path = os.path.join(model_dir, model_name)
                
                torch.save({
                    'model_state_dict': model.state_dict(),
                    'metrics': val_result,
                    'train_params': {
                        'lr': current_lr,
                        'weights': self.loss_weights.copy(),
                        'attempt': attempt+1,
                        'epochs': initial_epochs
                    },
                    'scaler_mean': trainer.scaler.mean_,
                    'scaler_scale': trainer.scaler.scale_
                }, checkpoint_path)
                print(f"\nğŸ† æ›´æ–°å…¨å±€æœ€ä½³æ¨¡å‹: {os.path.basename(checkpoint_path)}")
                saved_models.append(checkpoint_path)

                # æ›´æ–°æœ€ä½³æŒ‡æ ‡
                best_metrics.update({
                    'acc': val_result['acc'],
                    'max_drawdown': val_result['max_drawdown']
                })

            # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
            if val_result['acc'] >= self.auto_train_config['target_acc'] and \
               val_result['max_drawdown'] <= self.auto_train_config['max_drawdown']:
                print("âœ… è¾¾åˆ°è®­ç»ƒç›®æ ‡ï¼Œåœæ­¢è‡ªåŠ¨è®­ç»ƒ")
                return model
                
            attempt += 1
            
        # æœ€ç»ˆå¤„ç†
        print(f"\nâš ï¸ æœªè¾¾åˆ°ç›®æ ‡ï¼Œä½¿ç”¨æœ€ä½³å‚æ•°ç»„åˆé‡æ–°è®­ç»ƒ...")
        print("\n=== å€™é€‰æ¨¡å‹åˆ—è¡¨ ===")
        for path in saved_models:
            print(f"- {os.path.basename(path)}")
        
        # ç”¨æœ€ä½³å‚æ•°å¼ºåŒ–è®­ç»ƒ
        self.loss_weights = best_metrics['weights']
        final_model = self.train_model(symbol, epochs=int(initial_epochs*1.5), conservative_rate=0.5)
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_path = os.path.join(model_dir, f"{symbol}_final.pth")
        torch.save({
            'model_state_dict': final_model.state_dict(),
            'scaler_mean': trainer.scaler.mean_,
            'scaler_scale': trainer.scaler.scale_
        }, final_path)

        
        print(f"\nğŸ”¥ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³: {final_path}")
        return final_model

    def _init_time_windows(self, config):
        """åˆå§‹åŒ–æ—¶é—´çª—å£"""
        # ä»é…ç½®è·å–åˆå§‹æ—¶é—´
        self.train_start = pd.to_datetime(
            config.get('train_start') or self.cv_config['initial_train_start']
        )
        self.train_end = pd.to_datetime(
            config.get('train_end') or self.cv_config['initial_train_end']
        )
        self.val_start = pd.to_datetime(
            config.get('val_start') or self.cv_config['initial_val_start']
        )
        self.val_end = pd.to_datetime(
            config.get('val_end') or self.cv_config['initial_val_end']
        )
        self.test_start = pd.to_datetime(
            config.get('test_start') or self.cv_config['initial_test_start']
        )
        self.test_end = pd.to_datetime(
            config.get('test_end') or self.cv_config['initial_test_end']
        )

    def move_time_window(self):
        """ç§»åŠ¨æ—¶é—´çª—å£åˆ°ä¸‹ä¸€ä¸ªæŠ˜å """
        if self.current_fold >= self.total_folds - 1:
            return False
            
        # è®¡ç®—æ—¶é—´å¢é‡
        step = pd.DateOffset(months=self.cv_config.get('step_months', 2))
        train_window = pd.DateOffset(months=self.cv_config['train_window_months'])
        val_window = pd.DateOffset(months=self.cv_config['val_window_months'])
        
        # æ›´æ–°æ—¶é—´çª—å£
        self.train_start += step
        self.train_end = self.train_start + train_window
        self.val_start = self.train_end
        self.val_end = self.val_start + val_window
        
        self.current_fold += 1
        return True

    def create_sequences(self, data, labels):
        xs, ys = [], []
        total_length = len(data)
        
        # æ–°å¢è°ƒè¯•ä¿¡æ¯
        debug_samples = 3  # æ‰“å°å‰3ä¸ªæ ·æœ¬çš„æ—¶é—´ä¿¡æ¯
        print("\n=== åºåˆ—æ—¶é—´å¯¹é½è°ƒè¯• ===")
        
        # ç›´æ¥ä½¿ç”¨æ‰€æœ‰æœ‰æ•ˆæ ·æœ¬ï¼ˆå·²è¿‡æ»¤éäº¤æ˜“ä¿¡å·ï¼‰
        for i in range(total_length - self.seq_length - self.future_window + 1):
            label_idx = i + self.seq_length + self.future_window - 2
            if label_idx >= len(labels):
                continue
                
            # è·å–æ—¶é—´èŒƒå›´ï¼ˆå‡è®¾dataæ˜¯DataFrameï¼‰
            input_start = i
            input_end = i + self.seq_length
            label_position = label_idx
            

            timestamps = data.index if hasattr(data, 'index') else pd.date_range(start=self.train_start, periods=len(data), freq='H')
            # æ–°å¢æ—¶é—´æˆ³è®¡ç®—
            input_start_time = timestamps[i].strftime('%m-%d %H:%M')
            input_end_time = timestamps[i+self.seq_length-1].strftime('%m-%d %H:%M')
            label_time = timestamps[label_idx].strftime('%m-%d %H:%M')

            if i < 3:
                print(f"æ ·æœ¬{i} è¾“å…¥æ—¶æ®µ: {input_start_time} è‡³ {input_end_time}")
                print(f"æ ‡ç­¾æ—¶æ®µ: {label_time} (future_window={self.future_window}h)")
                print(f"æ ‡ç­¾å€¼: signal={labels[label_idx][0]}, return={labels[label_idx][3]:.8f}\n")
            
            xs.append(data[i:i+self.seq_length])
            ys.append([
                labels[label_idx][0],
                labels[label_idx][1],
                labels[label_idx][2],
                labels[label_idx][3]
            ])
        
        print(f"\nç”Ÿæˆæœ‰æ•ˆåºåˆ—æ•°é‡: {len(xs)} (æ€»æ•°æ®é•¿åº¦: {total_length})")
        return np.array(xs), np.array(ys)

    def prepare_and_save_features(self, symbol):
        def process_rolling_data(raw_data, window_size=24):
            processed = []
            raw_data = raw_data.sort_index()
            for i in range(window_size, len(raw_data)):
                window_start = raw_data.index[i - window_size]
                window_end = raw_data.index[i-1]
                window_data = raw_data.loc[window_start:window_end]
                if len(window_data) == window_size:
                    # ä»…ç”Ÿæˆç‰¹å¾ï¼ˆç§»é™¤æ ‡ç­¾ç”Ÿæˆï¼‰
                    features = prepare_features(window_data)
                    processed.append(features)
            return pd.concat(processed)

        print("\n=== åŠ è½½åŸå§‹æ•°æ® ===")
        train_raw = self.data_adapter.load_data(
            symbol=symbol,
            timeframe=self.timeframe,
            start=self.train_start,
            end=self.train_end,
            btc_symbol='BTC_USDT_USDT',
            eth_symbol='ETH_USDT_USDT'

        )
        val_raw = self.data_adapter.load_data(
            symbol=symbol,
            timeframe=self.timeframe,
            start=self.val_start,
            end=self.val_end,
            btc_symbol='BTC_USDT_USDT',
            eth_symbol='ETH_USDT_USDT'
        )
        test_raw = self.data_adapter.load_data(
            symbol=symbol,
            timeframe=self.timeframe,
            start=self.test_start,
            end=self.test_end,
            btc_symbol='BTC_USDT_USDT',
            eth_symbol='ETH_USDT_USDT'
        )
        print("\n=== å¤„ç†æ»šåŠ¨çª—å£ ===")
        train_df = process_rolling_data(train_raw)
        val_df = process_rolling_data(val_raw)
        test_df = process_rolling_data(test_raw)
        feature_dir = "model/features"
        os.makedirs(feature_dir, exist_ok=True)
        timestamp_str = datetime.datetime.now().strftime("%Y%m%d_%H%M")
        
        save_columns = ['datetime'] + get_feature_columns()
    
        train_df.reset_index().rename(columns={'index':'datetime'})\
            .to_csv(f"{feature_dir}/{symbol}_train_{timestamp_str}.csv", 
                    index=False,
                    columns=save_columns,  # ä½¿ç”¨åŒ…å«æ ‡ç­¾çš„åˆ—
                    encoding='utf-8-sig')
        
        val_df.reset_index().rename(columns={'index':'datetime'})\
            .to_csv(f"{feature_dir}/{symbol}_val_{timestamp_str}.csv",
                    index=False,
                    columns=save_columns,  # ä½¿ç”¨åŒ…å«æ ‡ç­¾çš„åˆ—
                    encoding='utf-8-sig')
        # ä¿å­˜æµ‹è¯•é›†ç‰¹å¾
        test_df.reset_index().rename(columns={'index':'datetime'})\
            .to_csv(f"{feature_dir}/{symbol}_test_{timestamp_str}.csv",
                    index=False,
                    columns=save_columns,
                    encoding='utf-8-sig')
        print(f"\nç‰¹å¾ä¿å­˜å®Œæˆ: {feature_dir}/{symbol}_[train/val/test]_{timestamp_str}.csv")
        # æ–°å¢æ ‡ç­¾ç”Ÿæˆæ­¥éª¤
        self._generate_labels_for_csv(feature_dir, symbol, timestamp_str)
        return train_df, val_df
    def _generate_labels_for_csv(self, feature_dir, symbol, timestamp_str):
        """ä¸ºå·²ä¿å­˜çš„CSVæ–‡ä»¶ç”Ÿæˆæ ‡ç­¾"""
        for mode in ['train', 'val', 'test']:
            file_path = f"{feature_dir}/{symbol}_{mode}_{timestamp_str}.csv"
            df = pd.read_csv(file_path, parse_dates=['datetime'])
            
            # ä½¿ç”¨å®Œæ•´æ•°æ®ç”Ÿæˆæ ‡ç­¾
            labels = generate_labels_from_csv(df, self.future_window)
            
            # åˆå¹¶æ ‡ç­¾åˆ°åŸå§‹æ•°æ®
            df = pd.concat([df, labels], axis=1)
            df.to_csv(file_path, index=False, encoding='utf-8-sig')

    def load_features(self, symbol, mode='train'):
        feature_dir = "model/features"
        file_pattern = f"{symbol}_{mode}_"
        latest_file = max(
            [f for f in os.listdir(feature_dir) if f.startswith(file_pattern)],
            key=lambda x: os.path.getctime(os.path.join(feature_dir, x))
        )
        
        df = pd.read_csv(os.path.join(feature_dir, latest_file), 
                        parse_dates=['datetime'],
                        index_col='datetime')
    
        df = df.dropna(subset=['signal', 'stop_loss', 'take_profit', 'return_pct'])

        #   # æ–°å¢è°ƒè¯•ä¿¡æ¯
        # print(f"\n=== ç‰¹å¾åˆ—éªŒè¯ ===")
        # print("CSVæ–‡ä»¶ä¸­çš„åˆ—:", df.columns.tolist())
        # print("æœŸæœ›çš„ç‰¹å¾åˆ—:", get_feature_columns())
        # print("ç¼ºå¤±çš„åˆ—:", list(set(get_feature_columns()) - set(df.columns)))
        if mode == 'train':
            self.scaler.fit(df[get_feature_columns()])
        
        features = self.scaler.transform(df[get_feature_columns()])  # ä½¿ç”¨è¿‡æ»¤åçš„ç‰¹å¾åˆ—
        labels = df[['signal', 'stop_loss', 'take_profit', 'return_pct']].values
        
        return features, labels, df
    def train_model(self, symbol, epochs=50, existing_model=None, conservative_rate=1.0):
        # åˆå¹¶åŠ è½½è®­ç»ƒé›†å’ŒéªŒè¯é›†
        train_features, train_labels, _ = self.load_features(symbol, 'train')
        val_features, val_labels, _ = self.load_features(symbol, 'val')
        
        # æå‰ç”Ÿæˆæ‰€æœ‰åºåˆ—
        X_train, y_train = self.create_sequences(train_features, train_labels)
        X_val, y_val = self.create_sequences(val_features, val_labels)
        
        # åˆ›å»ºæŒä¹…åŒ–çš„æ•°æ®åŠ è½½å™¨
        train_data = SOLDataset(X_train, y_train)
            # æ–°å¢ç¡¬ä»¶åŠ é€Ÿé…ç½®
        self.device_count = torch.cuda.device_count() if torch.cuda.is_available() else 0
        torch.backends.cudnn.benchmark = True  # å¯ç”¨cudnnè‡ªåŠ¨ä¼˜åŒ–å™¨
        self.num_workers = min(8, os.cpu_count()//1.5)  # æ ¹æ®CPUæ ¸å¿ƒæ•°è®¾ç½®å·¥ä½œè¿›ç¨‹æ•°
                # ä¿®æ”¹æ•°æ®åŠ è½½éƒ¨åˆ†ï¼ˆçº¦ç¬¬291è¡Œï¼‰
        self.train_loader = DataLoader(train_data, 
                                     batch_size=self.batch_size, 
                                     shuffle=True,
                                     num_workers=self.num_workers,
                                     pin_memory=True,
                                     persistent_workers=True,
                                     prefetch_factor=2)
        val_data = SOLDataset(X_val, y_val)
        self.val_loader = DataLoader(val_data, batch_size=self.batch_size, pin_memory=True)
        
        fold_results = {
            'best_epoch': 0,
            'best_val_return': -np.inf,
            'val_returns_per_epoch': []
        }
        # æ ‡ç­¾åˆ†å¸ƒåˆ†æ
        signal_dist = pd.Series(train_labels[:,0]).value_counts()
        print(f"\n=== æ ‡ç­¾åˆ†å¸ƒåˆ†æ ===")
        print(f"åšå¤šæ ·æœ¬: {signal_dist.get(1.0, 0)} ({signal_dist.get(1.0, 0)/len(train_labels):.2%})")
        print(f"åšç©ºæ ·æœ¬: {signal_dist.get(0.0, 0)} ({signal_dist.get(0.0, 0)/len(train_labels):.2%})")

        

        if existing_model:
            model = self.load_model(symbol, existing_model)
            # æ–°å¢ä¿å®ˆè®­ç»ƒé€»è¾‘
            print(f"\nå¯ç”¨ä¿å®ˆè®­ç»ƒæ¨¡å¼ (rate={conservative_rate})")
            base_lr = 0.001 * conservative_rate
            head_lr = 0.005 * conservative_rate
            # å¢å¼ºL2æ­£åˆ™åŒ–
            weight_decay = 0.01 / conservative_rate  
        else:
            model = EnhancedSOLModel(len(get_feature_columns())).to(self.device)
            base_lr = 0.001 * conservative_rate
            head_lr = 0.005 * conservative_rate
            # å¢å¼ºL2æ­£åˆ™åŒ–
            weight_decay = 0.01 / conservative_rate  

        optimizer = optim.AdamW([
            {'params': model.lstm.parameters(), 'lr': base_lr, 'weight_decay': weight_decay},
            {'params': model.attention.parameters(), 'lr': base_lr, 'weight_decay': weight_decay},
            {'params': model.signal_head.parameters(), 'lr': head_lr, 'weight_decay': weight_decay*2}
        ])
        
        # åœ¨å‚æ•°æ›´æ–°å¤„æ·»åŠ æ¢¯åº¦é™åˆ¶
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0 * conservative_rate)  # ç¼©å°æ¢¯åº¦è£å‰ªé˜ˆå€¼
        
        # æ–°å¢å­¦ä¹ ç‡è°ƒåº¦
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, 'min', patience=3, factor=0.5
        )

        positive_count = signal_dist.get(1.0, 1e-6)
        negative_count = signal_dist.get(0.0, 1e-6)
        pos_weight = torch.tensor([negative_count / positive_count]).to(self.device)
        signal_criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)

        # æ—©åœç›¸å…³å˜é‡åˆå§‹åŒ–
        self.no_improve_epochs = 0
        self.best_val_acc = 0
        self.best_val_return = -np.inf

        best_model_state = model.state_dict().copy()
        best_val_return = -np.inf

        best_val_metrics = {
            'acc': 0,
            'return': -np.inf,
            'drawdown': 1.0
        }
        best_max_drawdown = np.inf
        best_drawdown_model_state = None
        print("\n=== å¼€å§‹è®­ç»ƒ ===")
        for epoch in range(epochs):
            model.train()
            # current_dropout = min(
            #     self.adaptive_config['max_dropout'],
            #     0.3 + 0.4 * (epoch / epochs)  # éšè®­ç»ƒè¿›åº¦å¢åŠ dropout
            # )
            # model.lstm.dropout = current_dropout
            # model.attention.dropout = current_dropout
            
            # # æ·»åŠ è¾“å…¥æ•°æ®å™ªå£°
            # for batch_idx, (inputs, targets) in enumerate(tqdm(self.train_loader)):
            #     noise = torch.randn_like(inputs) * self.adaptive_config['noise_scale']
            #     inputs = inputs + noise
            total_loss = 0
            batch_returns = []  # æ–°å¢ï¼šè®°å½•æ¯ä¸ªbatchçš„æ”¶ç›Š
            for batch_idx, (inputs, targets) in enumerate(tqdm(self.train_loader, desc=f'Epoch {epoch+1}')):
                inputs, targets = inputs.to(self.device), targets.to(self.device)
                optimizer.zero_grad()
                
                # æ–°å¢æ”¶ç›Šç‡æƒé‡è®¡ç®—
                return_pct = targets[:, 3]  # æå–æ”¶ç›Šç‡æ•°æ®
                sample_weights = torch.exp(return_pct * 5)  # æŒ‡æ•°æ”¾å¤§é«˜æ”¶ç›Šæ ·æœ¬æƒé‡
                sample_weights = sample_weights / sample_weights.mean()  # æ–°å¢å½’ä¸€åŒ–
                # ä¿®æ”¹æŸå¤±è®¡ç®—éƒ¨åˆ†ï¼Œçº¦ç¬¬256è¡Œ
                outputs = model(inputs)
                base_loss = signal_criterion(outputs[:, 0], targets[:, 0].float())
                
                # ç„¶ååº”ç”¨æ ·æœ¬æƒé‡
                signal_loss = (base_loss * sample_weights).mean()

                # è®¡ç®—æ”¶ç›ŠæŸå¤±ï¼ˆä½¿ç”¨ç»å¯¹æ”¶ç›Šç‡å’Œä¿¡å·åŒ¹é…åº¦ï¼‰
                pred_signals = (torch.sigmoid(outputs[:, 0]) > 0.5).long()
                correct_direction = (pred_signals == targets[:, 0].long()).float()
                abs_returns = targets[:, 3]
                
                # æ–°å¢å®é™…æ”¶ç›Šè®¡ç®—ï¼ˆä¿®å¤ç¼ºå¤±å˜é‡ï¼‰
                actual_returns = correct_direction * return_pct
                return_loss = -torch.mean(actual_returns * sample_weights)  # åŠ æƒæ”¶ç›ŠæŸå¤±
                
                # æ–°å¢ä¸‰é¡¹æƒ©ç½šé¡¹
                smooth_loss = 0
                
                # ä½¿ç”¨detach()é¿å…å½±å“ä¸»æŸå¤±æ¢¯åº¦
                actual_returns_detached = actual_returns.detach()
                
                # 1. å¤æ™®æ¯”ç‡æƒ©ç½šï¼ˆé¼“åŠ±é«˜å¤æ™®ï¼‰
                sharpe_ratio = torch.mean(actual_returns_detached) / (torch.std(actual_returns_detached) + 1e-6)
                smooth_loss += 0.3 * (1 - sharpe_ratio)
                    # æ·»åŠ å®æ—¶è®¡ç®—æœ€å¤§å›æ’¤å’Œæ”¶ç›Šæ³¢åŠ¨æ€§      
                # 2. æœ€å¤§å›æ’¤æƒ©ç½šï¼ˆå®æ—¶è®¡ç®—ï¼‰
                # 2. æœ€å¤§å›æ’¤æƒ©ç½šï¼ˆå®æ—¶è®¡ç®—ï¼‰
                cumulative = torch.cumprod(1 + actual_returns, dim=0)
                peak = torch.cummax(cumulative, dim=0)[0]
                drawdown = (peak - cumulative) / (peak + 1e-6)
                smooth_loss += 0.5 * torch.max(drawdown)  # ç³»æ•°å¯è°ƒ
                # 3. æ”¶ç›Šæ³¢åŠ¨æ€§æƒ©ç½šï¼ˆæƒ©ç½šæ–¹å·®è¿‡å¤§ï¼‰
                return_variance = torch.var(actual_returns)
                smooth_loss += 0.2 * return_variance  # ç³»æ•°å¯è°ƒ
                
                # ç»„åˆæŸå¤±ï¼ˆè°ƒæ•´æ€»æŸå¤±å…¬å¼ï¼‰
                # print(f"signal_loss is =>{self.loss_weights['signal'] * signal_loss}, return_loss is =>{self.loss_weights['return'] * return_loss}, smooth_loss is =>{self.loss_weights['smooth'] * smooth_loss}, max_drawdown is =>{self.loss_weights['drawdown'] * max_drawdown}")
                loss = (
                    self.loss_weights['signal'] * signal_loss +
                    self.loss_weights['return'] * return_loss +
                    self.loss_weights['smooth'] * smooth_loss
                )
    
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                
                total_loss += loss.item()

                
            # # # ç»Ÿä¸€è¯„ä¼°è®­ç»ƒé›†å’ŒéªŒè¯é›†
            # eval_result = self._evaluate_model(model, self.train_loader)
            # train_acc = eval_result['acc']
            # train_return = eval_result['avg_return']

            val_result = self._evaluate_model(model, self.val_loader, is_val=True)
            current_val_return = val_result['total_return']
            # åŠ¨æ€è°ƒæ•´æŸå¤±æƒé‡ï¼ˆå½“å›æ’¤è¿‡å¤§æ—¶å¢å¼ºæ­£åˆ™åŒ–ï¼‰
            if val_result['max_drawdown'] > 0.2:
                self.loss_weights['smooth'] *= 1.2
                self.loss_weights['drawdown'] = min(self.loss_weights['drawdown'] + 0.1, 0.5)
            # æ–°å¢æœ€å¤§å›æ’¤åˆ¤æ–­é€»è¾‘
            current_drawdown = val_result['max_drawdown']
            if current_drawdown < best_max_drawdown:
                best_max_drawdown = current_drawdown
                best_drawdown_model_state = model.state_dict().copy()
                print(f"ğŸ¯ å‘ç°æ›´å°å›æ’¤æ¨¡å‹ (å›æ’¤: {best_max_drawdown:.2%})")
            # å½“éªŒè¯æ”¶ç›Šä¸‹é™æ—¶å¢å¼ºæƒé‡è¡°å‡
            if val_result['total_return'] < best_val_metrics['return']:
                for param_group in optimizer.param_groups:
                    param_group['weight_decay'] = min(
                        param_group['weight_decay'] * 1.1,
                        self.adaptive_config['weight_decay_range'][1]
                    )
                    
            # æ›´æ–°æœ€ä½³æŒ‡æ ‡
            if val_result['total_return'] > best_val_metrics['return']:
                best_val_metrics.update({
                    'acc': val_result['acc'],
                    'return': val_result['total_return'],
                    'drawdown': val_result['max_drawdown']
                })
                    
            #     # æ ¹æ®å‡†ç¡®ç‡è°ƒæ•´æ ·æœ¬æƒé‡
            #     if val_result['acc'] < 0.55:
            #         self.loss_weights['signal'] = min(self.loss_weights['signal'] * 1.1, 5)

            # ... åŸæœ‰è®°å½•éªŒè¯ç»“æœçš„ä»£ç  ...
            val_return = val_result['avg_return']
            val_acc = val_result['acc']
            # ä¿®æ”¹æ¨¡å‹ä¿å­˜æ¡ä»¶åˆ¤æ–­éƒ¨åˆ†
            if current_val_return > best_val_return:
                best_val_return = current_val_return
                best_model_state = model.state_dict().copy()
                self.no_improve_epochs = 0
                
                # æ–°å¢æ¨¡å‹ä¿å­˜é€»è¾‘
                if self.save_best_checkpoint:
                    checkpoint_path = os.path.join(
                        'C://Users//mazhao//Desktop//MAutoTrader//model',
                        f"{symbol}_epoch{epoch+1}_return{best_val_return:.2f}.pth"
                    )
                    torch.save({
                        'model_state_dict': model.state_dict(),
                        'scaler_mean': self.scaler.mean_,
                        'scaler_scale': self.scaler.scale_,
                        'val_return': best_val_return,
                        'epoch': epoch+1
                    }, checkpoint_path)
                    
                    # ä¿ç•™æœ€ä½³æ¨¡å‹è·¯å¾„
                    self.best_checkpoint_path = checkpoint_path
                    print(f"\nâœ… ä¿å­˜éªŒè¯é›†æœ€ä½³æ¨¡å‹: {checkpoint_path}")
            # if current_val_return > best_val_return:
            #     best_val_return = current_val_return
            #     best_model_state = model.state_dict().copy()
            #     self.no_improve_epochs = 0
                
            #     # å½“éªŒè¯é›†è¡¨ç°æå‡æ—¶ï¼Œå¢å¼ºæ­£åˆ™åŒ–
            #     for param_group in optimizer.param_groups:
            #         param_group['weight_decay'] *= 1.2  # å¢å¼ºL2æ­£åˆ™
            # else:
            #     self.no_improve_epochs += 1
            #     # é™ä½å­¦ä¹ ç‡
            #     for param_group in optimizer.param_groups:
            #         param_group['lr'] *= 0.95

            # # === æ—©åœæœºåˆ¶ ===
            # if self.no_improve_epochs >= self.early_stop_patience:
            #     print(f"æ—©åœè§¦å‘ï¼Œæ¢å¤æœ€ä½³æ¨¡å‹")
            #     model.load_state_dict(best_model_state)
            #     break

            val_return = val_result['avg_return']
            val_acc = val_result['acc']
            self.val_loader = DataLoader(SOLDataset(X_val, y_val), batch_size=self.batch_size)
            
            # åœ¨æ¯epochæœ«å°¾è®°å½•éªŒè¯ç»“æœ
            fold_results['val_returns_per_epoch'].append(val_return)
            if val_return > fold_results['best_val_return']:
                fold_results['best_val_return'] = val_return
                fold_results['best_epoch'] = epoch
            # åŒæ—¶ä¿®æ”¹æŠ¥å‘Šè¾“å‡ºéƒ¨åˆ†ï¼ˆçº¦ç¬¬301è¡Œï¼‰

            # ä¿®æ”¹æŠ¥å‘Šè¾“å‡ºæ ¼å¼
            print(f"\nEpoch {epoch+1} ç»¼åˆæŠ¥å‘Š:")
            print(f"[è®­ç»ƒé›†] æŸå¤±: {total_loss/len(self.train_loader):.4f}")
            # print(f"[è®­ç»ƒé›†] æœ€ç»ˆæ”¶ç›Š: {eval_result['total_return']:.2%} | æœ€å¤§å›æ’¤: {eval_result['max_drawdown']:.2%}")
            # print(f"[è®­ç»ƒé›†] æŸå¤±: {total_loss/len(self.train_loader):.4f} | å‡†ç¡®ç‡: {train_acc:.2%} | å¹³å‡æ”¶ç›Š: {train_return:.2%}")
            # print(f"[è®­ç»ƒé›†] é¢„æµ‹ä¿¡å· | åšå¤š: {eval_result['pred_long']} | åšç©º: {eval_result['pred_short']}")

            print(f"\n[éªŒè¯é›†] å‡†ç¡®ç‡: {val_acc:.2%} | å¹³å‡æ”¶ç›Š: {val_return:.2%}")
            print(f"[éªŒè¯é›†] æœ€ç»ˆæ”¶ç›Š: {val_result['total_return']:.2%} | æœ€å¤§å›æ’¤: {val_result['max_drawdown']:.2%}")
            print(f"[éªŒè¯é›†] é¢„æµ‹ä¿¡å· | åšå¤š: {val_result['pred_long']} | åšç©º: {val_result['pred_short']}") 
            # éªŒè¯é›†è¯„ä¼°åæ·»åŠ æ—©åœåˆ¤æ–­
            # è®­ç»ƒç»“æŸååˆ†æç‰¹å¾é‡è¦æ€§
            stability = val_result['stability_metrics']
            print(f"\n[éªŒè¯é›†] ä½æ•ˆçª—å£: {stability['low_efficiency_ratio']:.2%} | æ³¢åŠ¨è¯„åˆ†: {stability['volatility_score']:.2f}")
            print(f"[éªŒè¯é›†] æœ€å¤§è¿ç»­ç›ˆåˆ©: {stability['max_consecutive_profit']}å°æ—¶ | æœ€å¤§è¿ç»­äºæŸ: {stability['max_consecutive_loss']}å°æ—¶")
            self._plot_equity_curve(val_result['returns_series'], symbol)


            # # åŒé‡æ¡ä»¶åˆ¤æ–­ï¼ˆå‡†ç¡®ç‡æˆ–æ”¶ç›Šä»»ä¸€æå‡éƒ½è§†ä¸ºæœ‰æ”¹è¿›ï¼‰
            # if (current_acc > self.best_val_acc + 1e-4) or (current_return > self.best_val_return + 1e-4):
            #     self.best_val_acc = max(current_acc, self.best_val_acc)
            #     self.best_val_return = max(current_return, self.best_val_return)
            #     self.no_improve_epochs = 0
            #     # ä¿å­˜æœ€ä½³æ¨¡å‹
            #     best_model_state = model.state_dict().copy()
            # else:
            #     self.no_improve_epochs += 1
            #     print(f"æ—©åœè®¡æ•°å™¨: {self.no_improve_epochs}/{self.early_stop_patience}")

            # # æ—©åœæ£€æŸ¥
            # if self.no_improve_epochs >= self.early_stop_patience:
            #     print(f"\næ—©åœè§¦å‘ï¼åœ¨epoch {epoch+1} éªŒè¯é›†å‡†ç¡®ç‡({current_acc:.2%})å’Œæ”¶ç›Š({current_return:.2%})è¿ç»­{self.early_stop_patience}æ¬¡æœªæå‡")
            #     model.load_state_dict(best_model_state)  # æ¢å¤æœ€ä½³æ¨¡å‹
            #     break
                

        # è®°å½•äº¤å‰éªŒè¯ç»“æœ
        self.best_val_returns.append(fold_results['best_val_return'])
        self.best_epochs.append(fold_results['best_epoch'])
        
        # è¾“å‡ºå½“å‰æŠ˜å ç»“æœ
        print(f"\n=== äº¤å‰éªŒè¯æŠ˜å  {self.current_fold+1}/{self.total_folds} ===")
        print(f"æ—¶é—´çª—å£: Train({self.train_start.date()}~{self.train_end.date()})"
            f" Val({self.val_start.date()}~{self.val_end.date()})")
        print(f"æœ€ä½³epoch: {fold_results['best_epoch']} éªŒè¯æ”¶ç›Š: {fold_results['best_val_return']:.2%}")
        # è®­ç»ƒç»“æŸåæ¢å¤æœ€ä½³å›æ’¤æ¨¡å‹
        # if best_drawdown_model_state is not None:
        #     model.load_state_dict(best_drawdown_model_state)
        #     print(f"\nğŸ”¥ æœ€ç»ˆä½¿ç”¨æœ€å°å›æ’¤æ¨¡å‹ (å›æ’¤: {best_max_drawdown:.2%})")
    
        self._analyze_feature_importance()
        return model

    def _evaluate_model(self, model, data_loader, is_val = False):
        """ç»Ÿä¸€è¯„ä¼°å‡½æ•°ï¼Œæ–°å¢æ”¶ç›Šç‡è®¡ç®—å’Œç‰¹å¾é‡è¦æ€§åˆ†æ"""
        model.eval()
        all_preds = []
        all_targets = []
        total_returns = []
        all_max_probs = []
        feature_importance = np.zeros(len(get_feature_columns()))
        total_samples = 0
        all_signals = []  # æ–°å¢ä¿¡å·æ–¹å‘å­˜å‚¨
        fixed_loader = DataLoader(data_loader.dataset, 
                                batch_size=max(2, self.batch_size),
                                shuffle=False)

        # ä¿®æ”¹æ¢¯åº¦è®¡ç®—ä¸Šä¸‹æ–‡
        with torch.set_grad_enabled(True):
            for inputs, targets in fixed_loader:
                inputs = inputs.to(self.device).requires_grad_(True)
                targets = targets.to(self.device)
                
                # ä¿®æ”¹å‰å‘ä¼ æ’­æ–¹å¼
                outputs = model(inputs)
                # loss = outputs[:, 0].mean()  # ä½¿ç”¨æ›´æ˜ç¡®çš„æŸå¤±è®¡ç®—
                
                # # æ¢¯åº¦è®¡ç®—ä¼˜åŒ–
                # model.zero_grad()
                # loss.backward()
                
                # # è·å–æ¢¯åº¦å¹¶æ ‡å‡†åŒ–
                # gradients = inputs.grad.abs().cpu().numpy()
                # gradients = (gradients - gradients.min()) / (gradients.max() - gradients.min() + 1e-8)
                
                # # ä¿®æ”¹èšåˆæ–¹å¼ï¼ˆæŒ‰æ—¶é—´æ­¥åŠ æƒï¼‰
                # time_weights = np.linspace(0.5, 1.5, gradients.shape[1])  # è¿‘æœŸçš„æ—¶åºæ›´é‡è¦
                # weighted_grad = gradients * time_weights[:, np.newaxis]
                # feature_importance += weighted_grad.sum(axis=(0,1))
                # total_samples += len(inputs)

                # é¢„æµ‹å’Œæ”¶ç›Šè®¡ç®—éƒ¨åˆ†ä¿æŒæ— æ¢¯åº¦
                # === ä¿®æ”¹ä¿¡å·å¤„ç†é€»è¾‘ ===
                with torch.no_grad():
                    # è·å–å½“å‰é¢„æµ‹ä¿¡å·
                    current_preds = (torch.sigmoid(outputs[:, 0]) > 0.5).long()
                    true_signals = targets[:, 0].long()
                    return_pct = targets[:, 3]

                    # æœ‰æ•ˆæ€§è¿‡æ»¤ä¿æŒåŸæœ‰é€»è¾‘
                    valid_mask = (targets[:, 1] != 0.5) & (targets[:, 2] != 0.5) & (true_signals != -1)
                    valid_preds = current_preds[valid_mask]  # ä½¿ç”¨å½“å‰é¢„æµ‹ä¿¡å·
                    valid_true = true_signals[valid_mask]
                    valid_returns = return_pct[valid_mask]
                    all_signals.extend(valid_preds)  # æ–°å¢
                    # ä¿æŒä¸model_paintorç›¸åŒçš„æ”¶ç›Šç‡è°ƒæ•´é€»è¾‘
                    adjusted_returns = torch.where(
                        valid_preds == valid_true,
                        valid_returns,
                        -valid_returns
                    )
                    
                    # æ”¶é›†æœ‰æ•ˆæ•°æ®
                    total_returns.extend(adjusted_returns.cpu().numpy())
                    all_preds.extend(valid_preds.cpu().numpy())
                    all_targets.extend(valid_true.cpu().numpy())
                     # ä¿®æ”¹ä¿¡å·æ¦‚ç‡è®¡ç®—æ–¹å¼
                    prob_long = torch.sigmoid(outputs[:, 0])
                    prob_short = 1 - prob_long
                    max_probs = torch.maximum(prob_long, prob_short)
                    
                    # åªç»Ÿè®¡æœ‰æ•ˆæ ·æœ¬çš„æ¦‚ç‡
                    valid_max_probs = max_probs[valid_mask].cpu().numpy()
                    all_max_probs.extend(valid_max_probs)
                # æ–°å¢æ¦‚ç‡åˆ†å¸ƒåˆ†æ
            if len(all_max_probs) > 0:
                bins = [0.5, 0.55, 0.6, 0.7, 1.0]
                hist, _ = np.histogram(all_max_probs, bins=bins)
                total = len(all_max_probs)
                distribution = {
                    '50%-55%': hist[0]/total,
                    '55%-60%': hist[1]/total,
                    '60%-70%': hist[2]/total,
                    '70%+': hist[3]/total
                }
                print("\nä¿¡å·ç½®ä¿¡åº¦åˆ†å¸ƒ:")
                for k, v in distribution.items():
                    print(f"{k}: {v:.2%}")
        # === é‡æ–°è®¡ç®—å…³é”®æŒ‡æ ‡ ===
        returns_series = pd.Series(total_returns)
        if len(returns_series) == 0:
            print("è­¦å‘Šï¼šæ²¡æœ‰æœ‰æ•ˆäº¤æ˜“ä¿¡å·")
            return {
                'acc': 0, 'total_return': 0, 'avg_return': 0, 
                'sharpe': 0, 'max_drawdown': 0, 'win_rate': 0,
                'pred_long': 0, 'pred_short': 0, 'returns_series': returns_series
            }
        
        # è®¡ç®—ä¸ç­–ç•¥æŠ¥å‘Šä¸€è‡´çš„æŒ‡æ ‡
        total_return = returns_series.sum()
        valid_signals = pd.DataFrame({
            'pred': all_preds,
            'true': all_targets
        })
        
        # å‡†ç¡®ç‡è®¡ç®—
        acc = (valid_signals['pred'] == valid_signals['true']).mean()
        
        # ä¿¡å·ç»Ÿè®¡
        pred_long = valid_signals['pred'].eq(1).sum()
        pred_short = valid_signals['pred'].eq(0).sum()
        
        # é£é™©æŒ‡æ ‡è®¡ç®—ï¼ˆä¸model_paintorç›¸åŒé€»è¾‘ï¼‰
        equity_curve = self.initial_balance + np.cumsum(returns_series * self.initial_balance)
        peak = np.maximum.accumulate(equity_curve)
        drawdown = (peak - equity_curve) / peak
        max_drawdown = drawdown.max()
        
        sharpe_ratio = returns_series.mean() / returns_series.std() * np.sqrt(24*365) if returns_series.std() != 0 else 0
        win_rate = (returns_series > 0).mean()
        # æ–°å¢ç¨³å®šæ€§æŒ‡æ ‡è®¡ç®—
        def calculate_stability_metrics(returns):
            # 1. æ»šåŠ¨çª—å£æ”¶ç›Šç¨³å®šæ€§ï¼ˆ168å°æ—¶çª—å£ï¼‰
            rolling_7d = returns.rolling(168, min_periods=1).sum()
            # ç»Ÿè®¡ä½æ•ˆçª—å£ï¼ˆæ”¶ç›Š<5%ï¼‰
            low_efficiency = (rolling_7d < 0.05).sum() / len(returns)
            
            # 2. æ”¶ç›Šå¹³æ»‘åº¦ï¼ˆæ»šåŠ¨æ ‡å‡†å·®ï¼‰
            rolling_std = returns.rolling(24).std().dropna()
            volatility_score = 1 / (1 + rolling_std.mean())
            
            # 3. è¿ç»­æ”¶ç›Š/äºæŸå¤©æ•°ç»Ÿè®¡
            positive_streaks = []
            negative_streaks = []
            current_streak = 0
            current_sign = 0
            
            for r in returns:
                if r > 0:
                    new_sign = 1
                elif r < 0:
                    new_sign = -1 
                else:
                    new_sign = 0
                    
                if new_sign == current_sign:
                    current_streak += 1
                else:
                    if current_sign == 1:
                        positive_streaks.append(current_streak)
                    elif current_sign == -1:
                        negative_streaks.append(current_streak)
                    current_streak = 1
                    current_sign = new_sign
            
            # è®¡ç®—æœ€å¤§è¿ç»­å¤©æ•°
            max_positive = max(positive_streaks) if positive_streaks else 0
            max_negative = max(negative_streaks) if negative_streaks else 0
            
            return {
                'low_efficiency_ratio': low_efficiency,
                'volatility_score': volatility_score,
                'max_consecutive_profit': max_positive,
                'max_consecutive_loss': max_negative
            }
        return {
            'acc': acc,
            'total_return': total_return,
            'avg_return': returns_series.mean(),
            'sharpe': sharpe_ratio,
            'max_drawdown': max_drawdown,
            'win_rate': win_rate,
            'pred_long': pred_long,
            'pred_short': pred_short,
            'returns_series': returns_series,
            'signals': np.array(all_signals),
            'stability_metrics': calculate_stability_metrics(returns_series)
        }
    
    def _calculate_sharpe(self, returns_series):
        """è®¡ç®—å¹´åŒ–å¤æ™®æ¯”ç‡"""
        daily_mean = returns_series.mean() * 24  # å‡è®¾å°æ—¶æ•°æ®
        daily_std = returns_series.std() * np.sqrt(24)
        return daily_mean / daily_std if daily_std != 0 else 0

    def _calculate_max_drawdown(self, returns_series):
        """è®¡ç®—æœ€å¤§å›æ’¤å’Œå›æ’¤æŒç»­æ—¶é—´"""
        # ä¿®æ”¹ä¸ºä¸ç®€å•ç´¯åŠ ä¸€è‡´çš„è®¡ç®—æ–¹å¼
        equity_curve = self.initial_balance + np.cumsum(returns_series)  # ä½¿ç”¨ç´¯åŠ æ”¶ç›Š
        peak = np.maximum.accumulate(equity_curve)
        drawdown = (peak - equity_curve) / peak
        
        max_dd = drawdown.max()
        dd_duration = (drawdown == max_dd).sum()
        
        return max_dd, dd_duration

    def _calculate_smoothness(self, returns_series):
        """ä¿®æ”¹ä¸º72å°æ—¶çª—å£çš„æ»šåŠ¨æ”¶ç›Šæ ‡å‡†å·®"""
        if len(returns_series) < 168:
            return 0
        # è®¡ç®—72å°æ—¶ç´¯è®¡æ”¶ç›Šçš„æ³¢åŠ¨ç‡
        rolling_returns = returns_series.rolling(168).sum()
        return rolling_returns.std()

    def _calculate_max_drawdown(self, returns_series):
        """ä¿®æ”¹ä¸º72å°æ—¶çª—å£å†…çš„æœ€å¤§å›æ’¤"""
        if len(returns_series) < 168:
            return 0, 0
        
        max_drawdown = 0
        for i in range(len(returns_series)-168):
            window = returns_series[i:i+168]
            cumulative = np.cumprod(1 + window)
            peak = np.maximum.accumulate(cumulative)
            drawdown = (peak - cumulative)/peak
            max_drawdown = max(max_drawdown, drawdown.max())
        return max_drawdown, 0  # ç®€åŒ–æŒç»­æ—¶é—´è®¡ç®—

    def _calculate_profit_ratio(self, returns_series):
        """æ–°å¢ç¨³å®šæ€§æŒ‡æ ‡ï¼ˆè¿ç»­ç›ˆåˆ©/äºæŸæ¯”ä¾‹ï¼‰"""
        consecutive_pos = 0
        consecutive_neg = 0
        current_streak = 0
        
        for r in returns_series:
            if r > 0:
                current_streak = current_streak + 1 if current_streak >=0 else 1
            else:
                current_streak = current_streak - 1 if current_streak <=0 else -1
                
            if current_streak > consecutive_pos:
                consecutive_pos = current_streak
            elif current_streak < -consecutive_neg:
                consecutive_neg = abs(current_streak)
        
        stability_ratio = consecutive_pos / (consecutive_neg + 1e-6)
        return stability_ratio

    def _calculate_smoothness(self, returns_series):
        """è®¡ç®—æ”¶ç›Šå¹³æ»‘åº¦ï¼ˆæ»šåŠ¨æ³¢åŠ¨ç‡ï¼‰"""
        rolling_volatility = returns_series.rolling(24).std().dropna().mean()  # 24å°æ—¶çª—å£
        return 1 / (1 + rolling_volatility)  # æ³¢åŠ¨ç‡è¶Šä½å¹³æ»‘åº¦è¶Šé«˜

    def _calculate_profit_ratio(self, returns_series):
        """è®¡ç®—ç›ˆäºæ¯”"""
        gains = returns_series[returns_series > 0]
        losses = returns_series[returns_series < 0]
        return gains.mean() / abs(losses.mean()) if len(losses) > 0 else np.inf
    
    def _analyze_feature_importance(self):
        """åˆ†æå†å²ç‰¹å¾é‡è¦æ€§"""
        if not self.feature_importance_history:
            return
        
        # è®¡ç®—å¹³å‡é‡è¦æ€§
        avg_importance = np.mean(self.feature_importance_history, axis=0)
        feature_names = get_feature_columns()
        
        # ç”Ÿæˆç‰¹å¾æŠ¥å‘Š
        report = pd.DataFrame({
            'feature': feature_names,
            'avg_importance': avg_importance,
            'std_importance': np.std(self.feature_importance_history, axis=0)
        }).sort_values('avg_importance', ascending=False)
        
        # è‡ªåŠ¨è¯†åˆ«æ— æ•ˆç‰¹å¾ï¼ˆé‡è¦æ€§ä½äºå‡å€¼1ä¸ªæ ‡å‡†å·®ï¼‰
        threshold = report['avg_importance'].mean() - report['avg_importance'].std()
        low_importance_features = report[report['avg_importance'] < threshold]['feature'].tolist()
        
        # æ›´æ–°é»‘åå•
        self.feature_blacklist.update(low_importance_features)
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = "model/feature_analysis.csv"
        report.to_csv(report_path, index=False)
        print(f"\nç‰¹å¾åˆ†ææŠ¥å‘Šå·²ä¿å­˜è‡³ {report_path}")
        print("å»ºè®®ç§»é™¤ä»¥ä¸‹ä½æ•ˆç‰¹å¾:", low_importance_features)

    def _plot_equity_curve(self, returns, symbol):
        """ç»˜åˆ¶æ”¶ç›Šæ›²çº¿"""
        plt.figure(figsize=(12, 6))
        
        # ä¿®æ”¹ä¸ºç´¯åŠ è®¡ç®—
        cumulative_returns = np.cumsum(np.array(returns))  # ç›´æ¥ç´¯åŠ æ”¶ç›Šç‡
        equity_curve = self.initial_balance + cumulative_returns  # åˆå§‹æœ¬é‡‘ + ç´¯è®¡æ”¶ç›Š
        
        # ç»˜åˆ¶æ›²çº¿
        plt.plot(equity_curve, label='èµ„é‡‘æ›²çº¿', color='#2ca02c')
        plt.fill_between(range(len(equity_curve)), 
                        self.initial_balance, 
                        equity_curve,
                        color='#2ca02c', alpha=0.1)

        
        plt.title(f'{symbol} éªŒè¯é›†æ”¶ç›Šæ›²çº¿ (åˆå§‹æœ¬é‡‘ {self.initial_balance})')
        plt.xlabel('äº¤æ˜“æ¬¡æ•°')
        plt.ylabel('èµ„é‡‘ä»·å€¼ (USD)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # ä¿å­˜å›¾åƒ
        plot_dir = "model/plots"
        os.makedirs(plot_dir, exist_ok=True)
        plot_path = os.path.join(plot_dir, f"{symbol}_validation_curve.png")
        plt.savefig(plot_path, bbox_inches='tight', dpi=150)
        print(f"\næ”¶ç›Šæ›²çº¿å·²ä¿å­˜è‡³: {plot_path}")
        plt.close()

    def train(self, symbol='SOL_USDT_USDT', epochs=50, is_save=True, save_epoch_range=0, existing_model=None):
        if is_save:
            self.prepare_and_save_features(symbol)
        model = self.train_model(symbol, epochs, existing_model, self.conservative_rate)
        
        model_dir = "model"
        os.makedirs(model_dir, exist_ok=True)
        
        # æœ€ç»ˆæ¨¡å‹ä¿å­˜
        final_model_path = os.path.join(model_dir, f"{symbol}_nn_model.pth")
        torch.save({
            'model_state_dict': model.state_dict(),
            'scaler_mean': self.scaler.mean_,
            'scaler_scale': self.scaler.scale_,
            'trained_epochs': epochs  # è®°å½•æ€»è®­ç»ƒè½®æ•°
        }, final_model_path)

    def load_model(self, symbol, model):
        """åŠ è½½å·²æœ‰æ¨¡å‹å‚æ•°"""
        model_path = os.path.join("model", f"{symbol}_nn_model.pth")
        if os.path.exists(model_path):
            # æ·»åŠ å®‰å…¨å…¨å±€å˜é‡å£°æ˜
            torch.serialization.add_safe_globals([np.core.multiarray._reconstruct])
            
            checkpoint = torch.load(
                model_path, 
                map_location=self.device,
                weights_only=False  # æ˜¾å¼å…³é—­å®‰å…¨åŠ è½½
            )
            model.load_state_dict(checkpoint['model_state_dict'])
            print(f"\næˆåŠŸåŠ è½½å·²æœ‰æ¨¡å‹: {model_path}")
        return model

        # ç”Ÿæˆæ ‡ç­¾é€»è¾‘

# ä¿®æ”¹ä¸»å‡½æ•°éƒ¨åˆ†
if __name__ == "__main__":
    config_path = os.path.join(os.path.dirname(__file__), 'nnconfig.json')
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)['model_train']
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = SOLTrainer({
        'source': 'local',
        'data_path': config['data_path'],
        'timeframe': '1h',
        'batch_size': config['batch_size'],
        'seq_length': config['seq_length'],
        'existing_model': config.get('existing_model'),
        'cv_config': config.get('cv_config', {}),
        'conservative_rate': 0.01,
        'plot_validation': True,  # å¯ç”¨ç»˜å›¾åŠŸèƒ½
    })
    
    # # ä¿®æ”¹åçš„è®­ç»ƒå¾ªç¯
    # for symbol in config['symbols']:
    #     print(f"\n=== å¼€å§‹è‡ªåŠ¨è®­ç»ƒ {symbol} ===")
    #     # è°ƒç”¨è‡ªåŠ¨è®­ç»ƒæ–¹æ³•
    #     model = trainer.auto_train(
    #         symbol=symbol,
    #         initial_epochs=config.get('initial_epochs', 100)
    #     )
        
    #     # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    #     model_path = os.path.join("model", f"{symbol}_nn_model.pth")
    #     torch.save({
    #         'model_state_dict': model.state_dict(),
    #         'scaler_mean': trainer.scaler.mean_,
    #         'scaler_scale': trainer.scaler.scale_,
    #         'total_epochs': config.get('initial_epochs', 100)
    #     }, model_path)
    #     print(f"\nâœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")


    # ä¿®æ”¹åçš„è®­ç»ƒå¾ªç¯
    for symbol in config['symbols']:
        total_epochs = config.get('epochs', 50)
        model = None
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å·²æœ‰æ¨¡å‹
        model_path = os.path.join("model", f"{symbol}_nn_model.pth")
        if os.path.exists(model_path):
            load_choice = input("æ£€æµ‹åˆ°å·²æœ‰æ¨¡å‹ï¼Œæ˜¯å¦åŠ è½½ï¼Ÿ(y/n): ")
            if load_choice.lower() == 'y':
                base_model = EnhancedSOLModel(len(get_feature_columns())).to(trainer.device)
                model = trainer.load_model(symbol, base_model)
        
        # é¦–æ¬¡è®­ç»ƒæˆ–éœ€è¦é‡æ–°è®­ç»ƒ
        if model is None:
            print("\nå¼€å§‹å…¨æ–°è®­ç»ƒ...")
            model = trainer.train(symbol=symbol, epochs=total_epochs, is_save=False)
        else:
            print(f"\nç»§ç»­è®­ç»ƒ (å½“å‰æ€»è½®æ¬¡: {total_epochs})")
            additional_epochs = int(input("è¯·è¾“å…¥è¿½åŠ è®­ç»ƒæ¬¡æ•° (0é€€å‡º): "))
            if additional_epochs <= 0:
                break
            model = trainer.train(symbol=symbol, epochs=additional_epochs, 
                                    existing_model=model, is_save=False)
            total_epochs += additional_epochs
        
        # ä¿å­˜æ¨¡å‹ï¼ˆæ–°å¢ä¿å­˜æ ¡éªŒï¼‰
        if model:
            torch.save({
                'model_state_dict': model.state_dict(),
                'scaler_mean': trainer.scaler.mean_,
                'scaler_scale': trainer.scaler.scale_,
                'total_epochs': total_epochs
            }, model_path)
            print(f"æ¨¡å‹å·²æ›´æ–°: {model_path}")